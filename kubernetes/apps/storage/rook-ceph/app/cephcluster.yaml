---
# Example CephCluster CR
# Uncomment and customize this file after the rook-ceph operator is installed
# Then add it to kustomization.yaml resources
#
# Official documentation: https://rook.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/
#
# WARNING: Make sure sda is NOT your system/boot disk!
# This configuration uses sda disk on all nodes for Ceph storage.

apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: storage
spec:
  cephVersion:
    # Use latest stable Ceph version
    # Check available versions: https://quay.io/repository/ceph/ceph?tab=tags
    # Rook master branch requires Ceph 19.2.0-0 (squid) or higher
    # Using v20.2.0 (latest stable as of 2024)
    image: quay.io/ceph/ceph:v20.2.0
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  storage:
    config:
      # Use bluestore (default, but explicit for clarity)
      storeType: "bluestore"
    # Specify devices for each node (required when disk has partitions)
    # WARNING: This will use the entire sda disk in raw mode and may wipe partitions!
    # Make sure sda is NOT your system/boot disk!
    nodes:
      - name: "talos-1"
        devices:
          - name: "sda"
      - name: "talos-2"
        devices:
          - name: "sda"
      - name: "talos-3"
        devices:
          - name: "sda"
  mgr:
    count: 1
  dashboard:
    enabled: true
  monitoring:
    enabled: false
  network:
    provider: host
  crashCollector:
    disable: false
